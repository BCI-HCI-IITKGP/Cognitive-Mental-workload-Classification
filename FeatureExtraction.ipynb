{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pywt\n",
    "import pyedflib\n",
    "import numpy as np\n",
    "from spectrum import *\n",
    "from os import listdir\n",
    "from nitime import utils\n",
    "import scipy.stats as sp\n",
    "from os.path import isfile, join\n",
    "from nitime.viz import plot_tseries\n",
    "from matplotlib import pyplot as plt\n",
    "from nitime import algorithms as alg\n",
    "from nitime.timeseries import TimeSeries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features List!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names = ['Fractal Dimension','Coeffiecient of Variation','Mean of Vertex to Vertex Slope','Variance of Vertex to Vertex Slope',\n",
    "         'Hjorth_Activity','Hjorth_Mobility','Hjorth_Complexity',\n",
    "         'Kurtosis','2nd Difference Mean','2nd Difference Max',\n",
    "         'Skewness','1st Difference Mean','1st Difference Max',\n",
    "         'FFT Delta MaxPower','FFT Theta MaxPower','FFT Alpha MaxPower','FFT Beta MaxPower','Delta/Theta','Delta/Alpha','Theta/Alpha','(Delta+Theta)/Alpha',\n",
    "         '1Wavelet Approximate Mean','Wavelet Approximate Std Deviation','Wavelet Approximate Energy','Wavelet Detailed Mean','Wavelet Detailed Std Deviation','Wavelet Detailed Energy','Wavelet Approximate Entropy','Wavelet Detailed Entropy',\n",
    "         '2Wavelet Approximate Mean','Wavelet Approximate Std Deviation','Wavelet Approximate Energy','Wavelet Detailed Mean','Wavelet Detailed Std Deviation','Wavelet Detailed Energy','Wavelet Approximate Entropy','Wavelet Detailed Entropy',\n",
    "         '3Wavelet Approximate Mean','Wavelet Approximate Std Deviation','Wavelet Approximate Energy','Wavelet Detailed Mean','Wavelet Detailed Std Deviation','Wavelet Detailed Energy','Wavelet Approximate Entropy','Wavelet Detailed Entropy',\n",
    "         '4Wavelet Approximate Mean','Wavelet Approximate Std Deviation','Wavelet Approximate Energy','Wavelet Detailed Mean','Wavelet Detailed Std Deviation','Wavelet Detailed Energy','Wavelet Approximate Entropy','Wavelet Detailed Entropy',\n",
    "         '5Wavelet Approximate Mean','Wavelet Approximate Std Deviation','Wavelet Approximate Energy','Wavelet Detailed Mean','Wavelet Detailed Std Deviation','Wavelet Detailed Energy','Wavelet Approximate Entropy','Wavelet Detailed Entropy',\n",
    "         '6Wavelet Approximate Mean','Wavelet Approximate Std Deviation','Wavelet Approximate Energy','Wavelet Detailed Mean','Wavelet Detailed Std Deviation','Wavelet Detailed Energy','Wavelet Approximate Entropy','Wavelet Detailed Entropy',\n",
    "         '7Wavelet Approximate Mean','Wavelet Approximate Std Deviation','Wavelet Approximate Energy','Wavelet Detailed Mean','Wavelet Detailed Std Deviation','Wavelet Detailed Energy','Wavelet Approximate Entropy','Wavelet Detailed Entropy',\n",
    "         '8Wavelet Approximate Mean','Wavelet Approximate Std Deviation','Wavelet Approximate Energy','Wavelet Detailed Mean','Wavelet Detailed Std Deviation','Wavelet Detailed Energy','Wavelet Approximate Entropy','Wavelet Detailed Entropy',\n",
    "         '9Wavelet Approximate Mean','Wavelet Approximate Std Deviation','Wavelet Approximate Energy','Wavelet Detailed Mean','Wavelet Detailed Std Deviation','Wavelet Detailed Energy','Wavelet Approximate Entropy','Wavelet Detailed Entropy',\n",
    "         '10Wavelet Approximate Mean','Wavelet Approximate Std Deviation','Wavelet Approximate Energy','Wavelet Detailed Mean','Wavelet Detailed Std Deviation','Wavelet Detailed Energy','Wavelet Approximate Entropy','Wavelet Detailed Entropy',\n",
    "         '11Wavelet Approximate Mean','Wavelet Approximate Std Deviation','Wavelet Approximate Energy','Wavelet Detailed Mean','Wavelet Detailed Std Deviation','Wavelet Detailed Energy','Wavelet Approximate Entropy','Wavelet Detailed Entropy',\n",
    "         '12Wavelet Approximate Mean','Wavelet Approximate Std Deviation','Wavelet Approximate Energy','Wavelet Detailed Mean','Wavelet Detailed Std Deviation','Wavelet Detailed Energy','Wavelet Approximate Entropy','Wavelet Detailed Entropy',\n",
    "         '13Wavelet Approximate Mean','Wavelet Approximate Std Deviation','Wavelet Approximate Energy','Wavelet Detailed Mean','Wavelet Detailed Std Deviation','Wavelet Detailed Energy','Wavelet Approximate Entropy','Wavelet Detailed Entropy',\n",
    "         '14Wavelet Approximate Mean','Wavelet Approximate Std Deviation','Wavelet Approximate Energy','Wavelet Detailed Mean','Wavelet Detailed Std Deviation','Wavelet Detailed Energy','Wavelet Approximate Entropy','Wavelet Detailed Entropy',\n",
    "         'AR1','AR2','AR3','AR4','AR5','AR6','AR7','AR8','AR9','AR10','AR11','AR12','AR13','AR14','AR15','AR16','AR17','AR18',\n",
    "         'AR19','AR20','AR21','AR22','AR23','AR24','AR25','AR26','AR27','AR28','AR29','AR30','AR31','AR32','AR33','AR34','AR35','AR36','AR37','AR38','AR39','AR40','AR41','AR42'\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fractal Dimension!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fractal_dimension(Z, threshold=0.9):\n",
    "\n",
    "    # Only for 2d image\n",
    "    assert(len(Z.shape) == 2)\n",
    "\n",
    "    # From https://github.com/rougier/numpy-100 (#87)\n",
    "    def boxcount(Z, k):\n",
    "        S = np.add.reduceat(\n",
    "            np.add.reduceat(Z, np.arange(0, Z.shape[0], k), axis=0),\n",
    "                               np.arange(0, Z.shape[1], k), axis=1)\n",
    "\n",
    "        # We count non-empty (0) and non-full boxes (k*k)\n",
    "        return len(np.where((S > 0) & (S < k*k))[0])\n",
    "\n",
    "\n",
    "    # Transform Z into a binary array\n",
    "    Z = (Z < threshold)\n",
    "\n",
    "    # Minimal dimension of image\n",
    "    p = min(Z.shape)\n",
    "\n",
    "    # Greatest power of 2 less than or equal to p\n",
    "    n = 2**np.floor(np.log(p)/np.log(2))\n",
    "\n",
    "    # Extract the exponent\n",
    "    n = int(np.log(n)/np.log(2))\n",
    "\n",
    "    # Build successive box sizes (from 2**n down to 2**1)\n",
    "    sizes = 2**np.arange(n, 1, -1)\n",
    "\n",
    "    # Actual box counting with decreasing size\n",
    "    counts = []\n",
    "    for size in sizes:\n",
    "        counts.append(boxcount(Z, size))\n",
    "\n",
    "    # Fit the successive log(sizes) with log (counts)\n",
    "    coeffs = np.polyfit(np.log(sizes), np.log(counts), 1)\n",
    "    return -coeffs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coefficient of Varaition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def coeff_var(a):\n",
    "    b = a #Extracting the data from the 14 channels\n",
    "    output = np.zeros(len(b)) #Initializing the output array with zeros\n",
    "    k = 0; #For counting the current row no.\n",
    "    for i in b:\n",
    "        mean_i = np.mean(i) #Saving the mean of array i\n",
    "        std_i = np.std(i) #Saving the standard deviation of array i\n",
    "        output[k] = std_i/mean_i #computing coefficient of variation\n",
    "        k=k+1\n",
    "    return np.sum(output)/14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean of Vertex to Vertex Slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import heapq\n",
    "from scipy.signal import argrelextrema\n",
    "\n",
    "\n",
    "def first_diff(i):\n",
    "    b=i    \n",
    "    \n",
    "    out = np.zeros(len(b))\n",
    "    \n",
    "    for j in range(len(i)):\n",
    "        out[j] = b[j-1]-b[j]# Obtaining the 1st Diffs\n",
    "        \n",
    "        j=j+1\n",
    "        c=out[1:len(out)]\n",
    "    return c #returns first diff\n",
    "\n",
    "\n",
    "def slope_mean(p):\n",
    "    b = p #Extracting the data from the 14 channels\n",
    "    output = np.zeros(len(b)) #Initializing the output array with zeros\n",
    "    res = np.zeros(len(b)-1)\n",
    "    \n",
    "    k = 0; #For counting the current row no.\n",
    "    for i in b:\n",
    "        x=i\n",
    "        amp_max = i[argrelextrema(x, np.greater)[0]]\n",
    "        t_max = argrelextrema(x, np.greater)[0]\n",
    "        amp_min = i[argrelextrema(x, np.less)[0]]\n",
    "        t_min = argrelextrema(x, np.less)[0]\n",
    "        t = np.concatenate((t_max,t_min),axis=0)\n",
    "        t.sort()#sort on the basis of time\n",
    "\n",
    "        h=0\n",
    "        amp = np.zeros(len(t))\n",
    "        res = np.zeros(len(t)-1)\n",
    "        for l in range(len(t)):\n",
    "            amp[l]=i[t[l]]\n",
    "           \n",
    "        \n",
    "        amp_diff = first_diff(amp)\n",
    "        \n",
    "        t_diff = first_diff(t)\n",
    "        \n",
    "        for q in range(len(amp_diff)):\n",
    "            res[q] = amp_diff[q]/t_diff[q]         \n",
    "        output[k] = np.mean(res) \n",
    "        k=k+1\n",
    "    return np.sum(output)/14\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance of Vertex to Vertex Slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import heapq\n",
    "from scipy.signal import argrelextrema\n",
    "\n",
    "\n",
    "def first_diff(i):\n",
    "    b=i    \n",
    "    \n",
    "    out = np.zeros(len(b))\n",
    "    \n",
    "    for j in range(len(i)):\n",
    "        out[j] = b[j-1]-b[j]# Obtaining the 1st Diffs\n",
    "        \n",
    "        j=j+1\n",
    "        c=out[1:len(out)]\n",
    "    return c #returns first diff\n",
    "\n",
    "\n",
    "def slope_var(p):\n",
    "    b = p #Extracting the data from the 14 channels\n",
    "    output = np.zeros(len(b)) #Initializing the output array with zeros\n",
    "    res = np.zeros(len(b)-1)\n",
    "    \n",
    "    k = 0; #For counting the current row no.\n",
    "    for i in b:\n",
    "        x=i\n",
    "        amp_max = i[argrelextrema(x, np.greater)[0]]#storing maxima value\n",
    "        t_max = argrelextrema(x, np.greater)[0]#storing time for maxima\n",
    "        amp_min = i[argrelextrema(x, np.less)[0]]#storing minima value\n",
    "        t_min = argrelextrema(x, np.less)[0]#storing time for minima value\n",
    "        t = np.concatenate((t_max,t_min),axis=0) #making a single matrix of all matrix\n",
    "        t.sort() #sorting according to time\n",
    "\n",
    "        h=0\n",
    "        amp = np.zeros(len(t))\n",
    "        res = np.zeros(len(t)-1)\n",
    "        for l in range(len(t)):\n",
    "            amp[l]=i[t[l]]\n",
    "           \n",
    "        \n",
    "        amp_diff = first_diff(amp)\n",
    "        \n",
    "        t_diff = first_diff(t)\n",
    "        \n",
    "        for q in range(len(amp_diff)):\n",
    "            res[q] = amp_diff[q]/t_diff[q] #calculating slope        \n",
    "    \n",
    "        output[k] = np.var(res) \n",
    "        k=k+1#counting k\n",
    "    return np.sum(output)/14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hjorth    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hjorth(input):                                             # function for hjorth \n",
    "    realinput = input\n",
    "    hjorth_activity = np.zeros(len(realinput))\n",
    "    hjorth_mobility = np.zeros(len(realinput))\n",
    "    hjorth_diffmobility = np.zeros(len(realinput))\n",
    "    hjorth_complexity = np.zeros(len(realinput))\n",
    "    diff_input = np.diff(realinput)\n",
    "    diff_diffinput = np.diff(diff_input)\n",
    "    k = 0\n",
    "    for j in realinput:\n",
    "        hjorth_activity[k] = np.var(j)\n",
    "        hjorth_mobility[k] = np.sqrt(np.var(diff_input[k])/hjorth_activity[k])\n",
    "        hjorth_diffmobility[k] = np.sqrt(np.var(diff_diffinput[k])/np.var(diff_input[k]))\n",
    "        hjorth_complexity[k] = hjorth_diffmobility[k]/hjorth_mobility[k]\n",
    "        k = k+1\n",
    "    return np.sum(hjorth_activity)/14, np.sum(hjorth_mobility)/14, np.sum(hjorth_complexity)/14                       #returning hjorth activity, hjorth mobility , hjorth complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def kurtosis(a):\n",
    "    b = a # Extracting the data from the 14 channels\n",
    "    output = np.zeros(len(b)) # Initializing the output array with zeros (length = 14)\n",
    "    k = 0; # For counting the current row no.\n",
    "    for i in b:\n",
    "        mean_i = np.mean(i) # Saving the mean of array i\n",
    "        std_i = np.std(i) # Saving the standard deviation of array i\n",
    "        t = 0.0\n",
    "        for j in i:\n",
    "            t += (pow((j-mean_i)/std_i,4)-3)\n",
    "        kurtosis_i = t/len(i) # Formula: (1/N)*(summation(x_i-mean)/standard_deviation)^4-3\n",
    "        output[k] = kurtosis_i # Saving the kurtosis in the array created\n",
    "        k +=1 # Updating the current row no.\n",
    "    return np.sum(output)/14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Difference Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def secDiffMean(a):\n",
    "    b = a # Extracting the data of the 14 channels\n",
    "    output = np.zeros(len(b)) # Initializing the output array with zeros (length = 14)\n",
    "    temp1 = np.zeros(len(b[0])-1) # To store the 1st Diffs\n",
    "    k = 0; # For counting the current row no.\n",
    "    for i in b:\n",
    "        t = 0.0\n",
    "        for j in range(len(i)-1):\n",
    "            temp1[j] = abs(i[j+1]-i[j]) # Obtaining the 1st Diffs\n",
    "        for j in range(len(i)-2):\n",
    "            t += abs(temp1[j+1]-temp1[j]) # Summing the 2nd Diffs\n",
    "        output[k] = t/(len(i)-2) # Calculating the mean of the 2nd Diffs\n",
    "        k +=1 # Updating the current row no.\n",
    "    return np.sum(output)/14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Difference Max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def secDiffMax(a):\n",
    "    b = a # Extracting the data from the 14 channels\n",
    "    output = np.zeros(len(b)) # Initializing the output array with zeros (length = 14)\n",
    "    temp1 = np.zeros(len(b[0])-1) # To store the 1st Diffs\n",
    "    k = 0; # For counting the current row no.\n",
    "    t = 0.0\n",
    "    for i in b:\n",
    "        for j in range(len(i)-1):\n",
    "            temp1[j] = abs(i[j+1]-i[j]) # Obtaining the 1st Diffs\n",
    "        t = temp1[1] - temp1[0]\n",
    "        for j in range(len(i)-2):\n",
    "            if abs(temp1[j+1]-temp1[j]) > t :\n",
    "                t = temp1[j+1]-temp1[j] # Comparing current Diff with the last updated Diff Max\n",
    "\n",
    "        output[k] = t # Storing the 2nd Diff Max for channel k\n",
    "        k +=1 # Updating the current row no.\n",
    "    return np.sum(output)/14\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def skewness(arr):\n",
    "    data = arr \n",
    "    skew_array = np.zeros(len(data)) #Initialinling the array as all 0s\n",
    "    index = 0; #current cell position in the output array\n",
    "   \n",
    "    for i in data:\n",
    "        skew_array[index]=sp.stats.skew(i,axis=0,bias=True)\n",
    "        index+=1 #updating the cell position\n",
    "    return np.sum(skew_array)/14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### First Difference Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def first_diff_mean(arr):\n",
    "    data = arr \n",
    "    diff_mean_array = np.zeros(len(data)) #Initialinling the array as all 0s\n",
    "    index = 0; #current cell position in the output array\n",
    "   \n",
    "    for i in data:\n",
    "        sum=0.0#initializing the sum at the start of each iteration\n",
    "        for j in range(len(i)-1):\n",
    "            sum += abs(i[j+1]-i[j]) # Obtaining the 1st Diffs\n",
    "           \n",
    "        diff_mean_array[index]=sum/(len(i)-1)\n",
    "        index+=1 #updating the cell position\n",
    "    return np.sum(diff_mean_array)/14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### First Difference Max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def first_diff_max(arr):\n",
    "    data = arr \n",
    "    diff_max_array = np.zeros(len(data)) #Initialinling the array as all 0s\n",
    "    first_diff = np.zeros(len(data[0])-1)#Initialinling the array as all 0s \n",
    "    index = 0; #current cell position in the output array\n",
    "   \n",
    "    for i in data:\n",
    "        max=0.0#initializing at the start of each iteration\n",
    "        for j in range(len(i)-1):\n",
    "            first_diff[j] = abs(i[j+1]-i[j]) # Obtaining the 1st Diffs\n",
    "            if first_diff[j]>max: \n",
    "                max=first_diff[j] # finding the maximum of the first differences\n",
    "        diff_max_array[index]=max\n",
    "        index+=1 #updating the cell position\n",
    "    return np.sum(diff_max_array)/14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wavelet Fetures!\n",
    "### Approx Mean, Approx Std Deviation, Approx Energy, Detailed Mean, Detailed Std Deviation, Detailed Energy, Approx Entropy & Detailed Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wavelet_features(epoch,channels):\n",
    "    cA_values = []\n",
    "    cD_values = []\n",
    "    cA_mean = []\n",
    "    cA_std = []\n",
    "    cA_Energy =[]\n",
    "    cD_mean = []\n",
    "    cD_std = []\n",
    "    cD_Energy = []\n",
    "    Entropy_D = []\n",
    "    Entropy_A = []\n",
    "    wfeatures = []\n",
    "    for i in range(channels):\n",
    "        cA,cD=pywt.dwt(epoch[i,:],'coif1')\n",
    "        cA_values.append(cA)\n",
    "        cD_values.append(cD)\t\t#calculating the coefficients of wavelet transform.\n",
    "    for x in range(channels):   \n",
    "        cA_mean.append(np.mean(cA_values[x]))\n",
    "        wfeatures.append(np.mean(cA_values[x]))\n",
    "        \n",
    "        cA_std.append(abs(np.std(cA_values[x])))\n",
    "        wfeatures.append(abs(np.std(cA_values[x])))\n",
    "        \n",
    "        cA_Energy.append(abs(np.sum(np.square(cA_values[x]))))\n",
    "        wfeatures.append(abs(np.sum(np.square(cA_values[x]))))\n",
    "        \n",
    "        cD_mean.append(np.mean(cD_values[x]))\t\t# mean and standard deviation values of coefficents of each channel is stored .\n",
    "        wfeatures.append(np.mean(cD_values[x]))\n",
    "\n",
    "        cD_std.append(abs(np.std(cD_values[x])))\n",
    "        wfeatures.append(abs(np.std(cD_values[x])))\n",
    "        \n",
    "        cD_Energy.append(abs(np.sum(np.square(cD_values[x]))))\n",
    "        wfeatures.append(abs(np.sum(np.square(cD_values[x]))))\n",
    "        \n",
    "        Entropy_D.append(abs(np.sum(np.square(cD_values[x]) * np.log(np.square(cD_values[x])))))\n",
    "        wfeatures.append(abs(np.sum(np.square(cD_values[x]) * np.log(np.square(cD_values[x])))))\n",
    "        \n",
    "        Entropy_A.append(abs(np.sum(np.square(cA_values[x]) * np.log(np.square(cA_values[x]))))) \n",
    "        wfeatures.append(abs(np.sum(np.square(cA_values[x]) * np.log(np.square(cA_values[x])))))\n",
    "    return wfeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FFT Max Power - Delta, Theta, Alpha & Beta Band!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "\n",
    "def maxPwelch(data_win,Fs):\n",
    " \n",
    "    \n",
    "    BandF = [0.1, 3, 7, 12, 30]\n",
    "    PMax = np.zeros([14,(len(BandF)-1)]);\n",
    "    \n",
    "    for j in range(14):\n",
    "        f,Psd = signal.welch(data_win[j,:], Fs)\n",
    "        \n",
    "        for i in range(len(BandF)-1):\n",
    "            fr = np.where((f>BandF[i]) & (f<=BandF[i+1]))\n",
    "            PMax[j,i] = np.max(Psd[fr])\n",
    "    \n",
    "    return np.sum(PMax[:,0])/14,np.sum(PMax[:,1])/14,np.sum(PMax[:,2])/14,np.sum(PMax[:,3])/14\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shanon Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def shanon_entropy(labels): # Shanon Entropy\n",
    "    \"\"\" Computes entropy of 0-1 vector. \"\"\"\n",
    "    n_labels = len(labels)\n",
    "    counts = np.bincount(labels)\n",
    "    probs = counts[np.nonzero(counts)] / n_labels\n",
    "    n_classes = len(probs)\n",
    "\n",
    "    if n_classes <= 1:\n",
    "        return 0\n",
    "    return - np.sum(probs * np.log(probs)) / np.log(n_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Spectral Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy.fft import fft\n",
    "from numpy import zeros, floor, log10, log, mean, array, sqrt, vstack, cumsum, ones, log2, std\n",
    "from numpy.linalg import svd, lstsq\n",
    "import time\n",
    "\n",
    "def bin_power(X,Band,Fs):\n",
    "    C = fft(X)\n",
    "    C = abs(C)\n",
    "    Power =zeros(len(Band)-1);\n",
    "    for Freq_Index in xrange(0,len(Band)-1):\n",
    "        Freq = float(Band[Freq_Index])   ## Xin Liu\n",
    "        Next_Freq = float(Band[Freq_Index+1])\n",
    "        Power[Freq_Index] = sum(C[floor(Freq/Fs*len(X)):floor(Next_Freq/Fs*len(X))])\n",
    "    Power_Ratio = Power/sum(Power)\n",
    "    return Power, Power_Ratio\n",
    "\n",
    "\n",
    "def spectral_entropy(X, Fs, Power_Ratio = None):\n",
    "    \n",
    "    Band = [0.1, 3, 7, 12, 30]\n",
    "    if Power_Ratio is None:\n",
    "        Power, Power_Ratio = bin_power(X, Band, Fs)\n",
    "\n",
    "    Spectral_Entropy = 0\n",
    "    for i in xrange(0, len(Power_Ratio) - 1):\n",
    "        Spectral_Entropy += Power_Ratio[i] * log(Power_Ratio[i])\n",
    "    Spectral_Entropy /= log(len(Power_Ratio))     # to save time, minus one is omitted\n",
    "    print('Shape of Spectral Entropy = ',n.shape(Spectral_Entropy))\n",
    "    return -1 * Spectral_Entropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoregressive model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoregression - Yule Walker Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def autogressiveModelParameters(labels):\n",
    "    b_labels = len(labels)\n",
    "    feature = []\n",
    "    for i in range(14):\n",
    "        coeff, sig = alg.AR_est_YW(labels[i,:], 11,)\n",
    "        feature.append(coeff)\n",
    "    a = []     \n",
    "    for i in range(11):\n",
    "        a.append(np.sum(feature[:][i])/14)\n",
    "     \n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoregression  - Burg Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def autogressiveModelParametersBurg(labels):\n",
    "    feature = []\n",
    "    feature1 = []\n",
    "    model_order = 3\n",
    "    for i in range(14):\n",
    "        AR, rho, ref = arburg(labels[i], model_order)\n",
    "        feature.append(AR);\n",
    "    for j in range(14):\n",
    "        for i in range(model_order):\n",
    "            feature1.append(feature[j][i])\n",
    "\n",
    "    return feature1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Vector Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training-Data/Idle/Clean_EEG_Data_S07_5.csv\n",
      "Training-Data/Idle/Clean_EEG_Data_S4_1.csv\n",
      "Training-Data/Idle/Clean_EEG_Data_S10_5.csv\n",
      "Training-Data/Idle/Clean_EEG_Data_S2_1.csv\n",
      "Training-Data/Idle/Clean_EEG_Data_S3_1.csv\n",
      "Training-Data/Idle/Clean_EEG_Data_S09_5.csv\n",
      "Training-Data/Idle/Clean_EEG_Data_S5_1.csv\n",
      "Training-Data/Idle/Clean_EEG_Data_S1_1.csv\n",
      "Training-Data/Idle/Clean_EEG_Data_S08_5.csv\n",
      "Training-Data/Idle/Clean_EEG_Data_S11_5.csv\n",
      "Training-Data/1-Back/Clean_EEG_Data_S11_1.csv\n",
      "Training-Data/1-Back/Clean_EEG_Data_S08_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/baljeet/.local/lib/python2.7/site-packages/scipy/signal/spectral.py:772: UserWarning: nperseg = 256, is greater than input length = 98, using nperseg = 98\n",
      "  'using nperseg = {1:d}'.format(nperseg, x.shape[-1]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training-Data/1-Back/Clean_EEG_Data_S4_1.csv\n",
      "Training-Data/1-Back/Clean_EEG_Data_S07_1.csv\n",
      "Training-Data/1-Back/Clean_EEG_Data_S2_1.csv\n",
      "Training-Data/1-Back/Clean_EEG_Data_S5_1.csv\n",
      "Training-Data/1-Back/Clean_EEG_Data_S09_1.csv\n",
      "Training-Data/1-Back/Clean_EEG_Data_S1_1.csv\n",
      "Training-Data/1-Back/Clean_EEG_Data_S10_1.csv\n",
      "Training-Data/1-Back/Clean_EEG_Data_S3_1 .csv\n"
     ]
    }
   ],
   "source": [
    "idlefiles  = [f for f in listdir('Training-Data/Idle') if isfile(join('Training-Data/Idle', f))] \n",
    "OneBackfiles = [f for f in listdir('Training-Data/1-Back') if isfile(join('Training-Data/1-Back', f))]\n",
    "#TwoBackfiles = [f for f in listdir('Training-Data/2-Back') if isfile(join('Training-Data/2-Back', f))]\n",
    "#DualOneBackfiles = [f for f in listdir('Training-Data/Dual-1-Back') if isfile(join('Training-Data/Dual-1-Back', f))]\n",
    "#DualTwoBackfiles = [f for f in listdir('Training-Data/Dual-2-Back') if isfile(join('Training-Data/Dual-2-Back', f))]\n",
    "\n",
    "files = []\n",
    "\n",
    "for i in idlefiles:\n",
    "    files.append([i,'Idle'])\n",
    "    \n",
    "for i in OneBackfiles:\n",
    "    files.append([i,'1-Back'])\n",
    "\n",
    "#for i in TwoBackfiles:\n",
    "#    files.append([i,'2-Back'])\n",
    "\n",
    "#for i in DualOneBackfiles:\n",
    "#    files.append([i,'Dual-1-Back'])\n",
    "\n",
    "#for i in DualTwoBackfiles:\n",
    "#    files.append([i,'Dual-2-Back'])\n",
    "    \n",
    "    \n",
    "mypath = 'Training-Data/'\n",
    "csvfile = 'Features/features.csv'\n",
    "\n",
    "with open(csvfile, \"a\") as output:\n",
    "    writer = csv.writer(output, lineterminator='\\n')\n",
    "    writer.writerow(names)\n",
    "    for counter in range(len(files)):\n",
    "        subfolder =  files[counter][1]\n",
    "        tag = files[counter][1] \n",
    "        #print(tag)\n",
    "        data_path = mypath + subfolder +'/'+files[counter][0]\n",
    "        \n",
    "        print(data_path)\n",
    "        rows = csv.reader(open(data_path))\n",
    "        sigbufs = [l for l in rows]\n",
    "        sigbufs = np.array(sigbufs)\n",
    "        sigbufs = sigbufs.transpose()\n",
    "        sigbufs = sigbufs.astype(float)\n",
    "\n",
    "        for i in np.arange(1,150,3):\n",
    "            features = []\n",
    "            epoch = sigbufs[:,i*128:(i+3)*128]\n",
    "            if len(epoch[0]) == 0:\n",
    "                break\n",
    "            \n",
    "            # Fractal Dimension\n",
    "            features.append(fractal_dimension(epoch))\n",
    "            \n",
    "            #Coeffeicient of Variation\n",
    "            features.append(coeff_var(epoch))\n",
    "\n",
    "            #Mean of Vertex to Vertex Slope\n",
    "            features.append(slope_mean(epoch))\n",
    "            \n",
    "            #Variance of Vertex to Vertex Slope\n",
    "            features.append(slope_var(epoch))\n",
    "       \n",
    "            #Hjorth Parameters\n",
    "            feature_list = hjorth(epoch)\n",
    "            for feat in feature_list:\n",
    "                features.append(feat)\n",
    "            \n",
    "            #Kurtosis\n",
    "            features.append(kurtosis(epoch))\n",
    "\n",
    "            #Second Difference Mean\n",
    "            features.append(secDiffMean(epoch))\n",
    "\n",
    "            #Second Difference Max\n",
    "            features.append(secDiffMax(epoch))\n",
    "               \n",
    "            #Skewness\n",
    "            features.append(skewness(epoch))\n",
    " \n",
    "            #First Difference Mean\n",
    "            features.append(first_diff_mean(epoch))\n",
    "\n",
    "            #First Difference Max\n",
    "            features.append(first_diff_max(epoch))\n",
    "            \n",
    "            #FFT Max Power - Delta, Theta, Alpha & Beta Band!\n",
    "            feature_list  =  maxPwelch(epoch,128)            \n",
    "            for feat in feature_list:\n",
    "                features.append(feat)\n",
    "            #FFT Frequency Ratios\n",
    "            features.append(feature_list[0]/feature_list[1])\n",
    "            features.append(feature_list[0]/feature_list[2])\n",
    "            features.append(feature_list[1]/feature_list[3])\n",
    "            features.append((feature_list[0] + feature_list[1])/feature_list[2])\n",
    "            \n",
    "            #Wavelet Fetures!      \n",
    "            feature_list = wavelet_features(epoch,14)\n",
    "            for feat in feature_list:\n",
    "                features.append(feat)\n",
    "                    \n",
    "            #Autoregressive model Coefficients\n",
    "            feature_list = autogressiveModelParametersBurg(epoch)\n",
    "            for feat in feature_list:\n",
    "                features.append(feat.real)\n",
    "       \n",
    "            \"\"\"   \n",
    "            #Shanon Entropy\n",
    "            abc = shanon_entropy(epoch)\n",
    "            print('Shape SE = ',np.shape(abc))\n",
    "            features.append(abc)\n",
    "            \n",
    "            #Spectral Entropy\n",
    "            feature_list = spectral_entropy(epoch,128)\n",
    "            for feat in feature_list:\n",
    "                features.append(feat)        \n",
    "            \"\"\"\n",
    "            \n",
    "            tag\n",
    "            if tag == 'Idle':\n",
    "                myClass = 1\n",
    "            \n",
    "            if tag == '1-Back':\n",
    "                myClass = 2\n",
    "            \n",
    "#            if tag == '2-Back':\n",
    "#               myClass = 3\n",
    "\n",
    "#            if tag == 'Dual-1-Back':\n",
    "#                myClass = 4\n",
    "            \n",
    "#            if tag == 'Dual-2-Back':\n",
    "#                myClass = 5\n",
    "            \n",
    "            features.append(myClass);        \n",
    "            writer.writerow(features) \n",
    "        \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r = csv.reader(open('Features/features.csv')) \n",
    "lines = [l for l in r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(lines[1])-1):  \n",
    "    columns = []\n",
    "    for j in range(1,len(lines)):\n",
    "        columns.append(float(lines[j][i]))\n",
    "    mean = np.mean(columns,axis = 0)\n",
    "    #print('\\nMean = ',mean)\n",
    "    std_dev  = np.std(columns,axis = 0)\n",
    "    #print('\\nSTD Deviation = ',std_dev)\n",
    "    for j in range(1,len(lines)):\n",
    "        lines[j][i] = (float(lines[j][i])-mean)/std_dev\n",
    "\n",
    "writer = csv.writer(open('Features/Normalizedfeatures.csv', 'wb'))\n",
    "writer.writerows(lines)\n",
    "print('Done!')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
